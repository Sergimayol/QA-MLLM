<div align="center">

<img src="./docs/assets/qa-mllm.png" height="482" width="1447">

# QA-MLLM (Question Answering - Multiple Large Language Models)

</div>

## Introduction

This project is a question answering system based on huggingface transformers. It is a combination of a multiple large language models (MLLM) in cascade in order to be able to answer quuestion from any language and from any input type, such as pdf, html, docx, images, videos, audio, etc.

## Installation

Work in progress ...

## Usage

Work in progress ...

## Development

```bash
conda env create -f environment.yml
```

```bash
conda activate qa-mlm
```

## In progress

- [ ] Allow to use pdf as context input
- [ ] Allow to use html as context input
- [ ] Allow to use docx as context input
- [ ] Allow to use images as context input
- [ ] Allow to use videos as context input
- [ ] Allow to use audio as context input
- [ ] Allow to use other languages as context input
- [ ] Create CLI application
- [ ] Create GUI application
- [ ] Allow to specify the model to use
- [ ] Allow hot swapping of models
- [ ] Allow hot swapping of tokenizer
- [ ] Allow hot swapping of context input

## License

This project is licensed under the terms of the MIT license. See [LICENSE](LICENSE) for additional details.
